{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    " \n",
    "# Decision Trees\n",
    " \n",
    "_Authors: Joseph Nelson, David Yerrington, Matt Brems, Jeff Hale_\n",
    "\n",
    "*Some adapted from Chapter 8 of ISLR*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "After this lesson you will be able to:\n",
    "\n",
    "- Explain how a decision tree is created.\n",
    "- Build a decision tree model in scikit-learn.\n",
    "- Tune a decision tree model and explain how tuning effects the model.\n",
    "- Interpret a tree diagram.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "- Non-parametric models - what does that mean?\n",
    "- Can be used for regression or classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree benefits:\n",
    "\n",
    "- Explainable\n",
    "- Interpretable\n",
    "- Visualizable\n",
    "- Basis for more sophisticated models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What will we get for dinner?\n",
    "\n",
    "|$Y = $ Food|$X_1 = $ Weather|$X_2 = $ Day|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|   Indian  |      Rainy     |   Weekday  |\n",
    "|   Sushi   |      Sunny     |   Weekday  |\n",
    "|   Indian  |      Rainy     |   Weekend  |\n",
    "|  Mexican  |      Sunny     |   Weekend  |\n",
    "|   Indian  |      Rainy     |   Weekday  |\n",
    "|  Mexican  |      Sunny     |   Weekend  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>It's a rainy day. Based on our past orders, what do you think we'll order?</summary>\n",
    "\n",
    "- Indian food.\n",
    "- In 100% of past cases where the weather is rainy, we've eaten Indian food!\n",
    "\n",
    "|$Y = $ Food|$X_1 = $ Weather|$X_2 = $ Day|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|   Indian  |      Rainy     |   Weekday  |\n",
    "|   Indian  |      Rainy     |   Weekend  |\n",
    "|   Indian  |      Rainy     |   Weekday  |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>It's a sunny day. Based on our past orders, what do you think we'll order?</summary>\n",
    "\n",
    "- Either Sushi or Mexican food... but we can't say with certainty whether we'd eat sushi or Mexican food.\n",
    "- Based on our past orders, we eat sushi on 1/3 of sunny days and we eat Mexican food on 2/3 of sunny days.\n",
    "- If I **had** to make a guess here, I'd probably predict Mexican food, but we may want to use additional information to be certain.\n",
    "\n",
    "|$Y = $ Food|$X_1 = $ Weather|$X_2 = $ Day|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|   Sushi   |      Sunny     |   Weekday  |\n",
    "|  Mexican  |      Sunny     |   Weekend  |\n",
    "|  Mexican  |      Sunny     |   Weekend  |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>It's a sunny day that also happens to be a weekend. Based on our past orders, what do you think we'll order?</summary>\n",
    "\n",
    "- Mexican food.\n",
    "- In 100% of past cases where the weather is sunny and where it's a weekend, we've eaten Mexican food!\n",
    "\n",
    "|$Y = $ Food|$X_1 = $ Weather|$X_2 = $ Day|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|  Mexican  |      Sunny     |   Weekend  |\n",
    "|  Mexican  |      Sunny     |   Weekend  |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Decision Trees: Overview\n",
    "\n",
    "A decision tree for classification:\n",
    "- takes a dataset consisting of $X$ and $Y$ data, \n",
    "- finds rules based on our $X$ data that splits our data into smaller datasets such that\n",
    "- by the bottom of the tree, the values $Y$ in each \"leaf node\" are as \"pure\" as possible.\n",
    "\n",
    "We frequently see decision trees represented by a graph.\n",
    "\n",
    "<img src=\"images/order_food_dt.png\" alt=\"order_food\" width=\"750\"/>\n",
    "\n",
    "- (This image was created using [Draw.io](https://www.draw.io/).)\n",
    "\n",
    "### Terminology\n",
    "Decision trees look like upside down trees. \n",
    "- The top node is the \"root node,\" through which all of our observations are passed.\n",
    "- At each internal split, our dataset is partitioned.\n",
    "- A \"parent\" node is split into two \"child\" nodes.\n",
    "- At each of the \"leaf nodes\" (colored orange), we contain a subset of records that are as pure as possible.\n",
    "    - In this food example, each leaf node is perfectly pure. Once we get to a leaf node, every observation in that leaf node has the exact same value of $Y$!\n",
    "    - There are ways to quantify the idea of \"purity\" here so that we can let our computer do most of the tree-building (model-fitting) process... we'll come back to this later.\n",
    "\n",
    "\n",
    "- [DecisionTreeClassifier Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "- [DecisionTreeRegressor Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Purity in Decision Trees\n",
    "\n",
    "When quantifying how \"pure\" a node is, we want to see what the distribution of $Y$ is in each node, then summarize this distribution with a number.\n",
    "\n",
    "<img src=\"images/order_food_dt.png\" alt=\"order_food\" width=\"750\"/>\n",
    "\n",
    "---\n",
    "#### For a continuous $y$ (i.e. using a decision tree to predict income), the default option is **mean squared error**.\n",
    "- This is the `criterion = 'mse'` argument in `DecisionTreeRegressor`.\n",
    "- When the decision tree is figuring out which split to make at a given node, it picks the split that maximizes the drop in MSE from the parent node to the child node.\n",
    "    \n",
    "    \n",
    "---\n",
    "#### For a discrete $y$, the default option is the **Gini impurity**, a measure of the homogeneity of the outcome class. 0 means totally homogeneous.\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "\\text{Gini impurity (2 classes)} &=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 \\\\\n",
    "\\text{Gini impurity (3 classes)} &=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 - P(\\text{class 3})^2 \\\\\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "Where P = probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ['Indian', 'Sushi', 'Indian', 'Mexican', 'Indian', 'Mexican']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What is the Gini impurity of a node when every y is from the same class?</summary>\n",
    "    \n",
    "- Our Gini impurity is zero.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "&=& 1 - P(\\text{class 1})^2 \\\\\n",
    "&=& 1 - 1^2 \\\\\n",
    "&=& 1 - 1 \\\\\n",
    "&=& 0\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What is the Gini impurity of a node when we have two classes, each with two items?</summary>\n",
    "    \n",
    "- Our Gini impurity is 0.5.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "&=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 \\\\\n",
    "&=& 1 - \\left(\\frac{1}{2}\\right)^2 - \\left(\\frac{1}{2}\\right)^2 \\\\\n",
    "&=& 1 - \\frac{1}{4} - \\frac{1}{4} \\\\\n",
    "&=& \\frac{1}{2}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What is the Gini impurity of a node when we have two classes, each with three items?</summary>\n",
    "    \n",
    "- Our Gini impurity is 0.5.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "&=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 \\\\\n",
    "&=& 1 - \\left(\\frac{1}{2}\\right)^2 - \\left(\\frac{1}{2}\\right)^2 \\\\\n",
    "&=& 1 - \\frac{1}{4} - \\frac{1}{4} \\\\\n",
    "&=& \\frac{1}{2}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### How does a CLASSIFICATION decision tree decide which variable to split on?\n",
    "\n",
    "1. At each node, consider the SUBSET of our DataFrame that exists at that node.\n",
    "1. Iterate through EACH feature that could potentially split the data.\n",
    "1. Calculate the Gini impurity for EACH possible split.\n",
    "1. Select the feature split that DROPS the Gini impurity to the lowest level.\n",
    "1. Repeat\n",
    "\n",
    "\n",
    "\n",
    "A decision tree makes the best short-term decision by optimizing at each node individually. _This might mean that our tree isn't optimal in the long run!_ \n",
    "\n",
    "Decision tree fitting use a **greedy** algorithm because it makes locally optimal decisions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Regression Trees üå≤\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## How Does a Computer Build a regression Tree? \n",
    "\n",
    "It has to do something different with a continuous y.\n",
    "\n",
    "\n",
    "Recursive binary splitting.\n",
    "\n",
    "1. Begin at the top of the tree.\n",
    "2. For **every feature**, examine **every possible cutpoint**, and choose the feature and cutpoint so that the resulting tree has the **lowest possible mean squared error (MSE)**. Make that split.\n",
    "3. Examine the two resulting regions. Once again, make a **single split** (in one of the regions) to minimize the MSE.\n",
    "4. Keep repeating Step 3 until a **stopping criterion** is met:\n",
    "    - Maximum tree depth (maximum number of splits required to arrive at a leaf).\n",
    "    - Minimum number of observations in a leaf.\n",
    "\n",
    "Remember this is a greedy algorithm (locally optimimal).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in vehicle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/vehicles_train.csv'\n",
    "train = pd.read_csv(path)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make X and y. Use year, miles, and doors to pedict price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=['price', 'vtype'])\n",
    "y = train['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make train and test sets out of the train data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a null baseline model and calculate the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Build a regression tree\n",
    "At every split, it will look at each feature for every potential cutpoint. It will choose the cutpoint that produces the lowest MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate a DecisionTreeRegressor (with random_state=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Tree Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `plot_tree()` function. You may need to install a package in your conda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Every tree is just a graph in disguise! üå≥\n",
    "\n",
    "A graph is made of nodes and edges (boxes and lines). A node is also called a vertex.\n",
    "\n",
    "A tree is a graph with one node with no incoming edges (the root). Each other node has exactly one incoming edge (to its parent node)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the graph:\n",
    "\n",
    "- **rule:** Rule used to split that node (go left if true, go right if false).\n",
    "- **samples:** Number of observations in that node before splitting.\n",
    "- **mse:** MSE calculated by comparing the actual response values in that node against the mean response value in that node.\n",
    "- **value:** Mean target value in that node (predicted value if a terminal node)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does a complete tree mean in terms of bias/variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"#tuning-tree\"></a>\n",
    "## Tuning a Regression Tree\n",
    "\n",
    "Let's try to reduce the MSE by tuning the **max_depth** parameter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try a few different values for `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Find the most important features ‚≠êÔ∏è\n",
    "We don't have coefficients, but we have a feature importance metric.\n",
    "#### _Gini importance_ of each feature: the (scaled, between 0 and 1) total reduction of error brought by that feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the `feature_importances_` attribute through the `best_estimator_` attribute of the GridSearchCV object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which feature does the model think is most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens when making predictions for new data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import **new** unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the tree diagram above, what predictions will the model make for each observation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the  fitted model to make predictions on final holdout testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Classification Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Index\n",
    "\n",
    "- Gini index makes splits that **increase node purity**, even if that split does not change the classification error rate.\n",
    "- Node purity is important because we're interested in the **class proportions** in each region. That's how we calculate the **predicted probability** of each class.\n",
    "- scikit-learn's default splitting criteria for classification trees is gini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Building a Classification Tree in `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build a classification tree using the Titanic survival data set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/titanic.csv'\n",
    "titanic = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Survived:** 0=died, 1=survived (response variable)\n",
    "- **Pclass:** 1=first class, 2=second class, 3=third class\n",
    "- **Sex:** female, male\n",
    "- **Age:** Numeric value\n",
    "- **Embarked:** C or Q or S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['Pclass', 'Age']\n",
    "\n",
    "X = titanic[feature_cols]\n",
    "y = titanic['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, for scikit-learn estimators, all our feature columns need to encoded numerically, but decision trees in scikit-learn will work with categorical data. But some scoring functions won't. ‚òπÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill missing values with KNN Imputation\n",
    "Should standard scale first. We'll just go with this for now and add standard scaling later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Fit a classification tree with max_depth=2 on all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a  visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the feature importances (the Gini index at each node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute all the metrics and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did our model do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### KNN imputer for missing values\n",
    "\n",
    "KNN imputing is based on distance, so we want to scale our data before imputing.\n",
    "\n",
    "Make a pipeline with StandardScaler, KNNImputer, and a Decision Tree. GridSearch over two hyperparameters of your choosing for the Decision Tree. Max depth is a good one to help our model avoid overfitting. üôÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate performance on the test set\n",
    "\n",
    "Plot the confusion matrix and the best tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### Disadvantages of decision trees:\n",
    "\n",
    "- **Not that good (alone):** Their performance is (generally) not competitive with the best supervised learning methods.\n",
    "- **Prone to overfitting:** They can easily overfit the training data (tuning is required).\n",
    "- **Often have high variance**: Small variations in the data can result in a completely different tree.\n",
    "- **Sensitive to class imbalance:** They don't tend to work well if the classes are highly unbalanced.\n",
    "- **Need number of observations:** They don't tend to work well with very small data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Cool! You've seen the basics of decision trees! üéâ\n",
    "\n",
    "Decision Trees are the foundation for a very powerful family models you'll learn about soon.\n",
    "\n",
    "### Check for Understanding\n",
    "\n",
    "- What are edges in a graph?\n",
    "- What is the top node in a decision tree called?\n",
    "- What are the bottom nodes called?\n",
    "- What's the most common splitting criterion for classification trees?\n",
    "- What's the most default splitting criterion for regression trees?\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

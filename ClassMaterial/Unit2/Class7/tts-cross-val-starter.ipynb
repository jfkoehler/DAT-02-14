{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/GA-logo.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Cross-Validation Lesson\n",
    "\n",
    "_Authors: Dave Yerrington (SF), Joseph Nelson (DC), Kiefer Katovich (SF), Riley Dallas(AUS), Adi Bronshtein (Live Online), Jeff Hale (Live Online)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- **Describe** train/test split\n",
    "- **Describe**  cross-validation.\n",
    "- **Explain** how these validation techniques differ and why we want to use them.\n",
    "- **Split** data into testing and training sets using train/test split and train, evaluate, and interpret\n",
    "- **Use** cross-validation to choose your hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/overfitting.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's wrong with the first model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's wrong with the third model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The middle \"Goldilocks\" model is a good compromise.**\n",
    "- It approximates the complexity of the true model and does not model random noise in our sample as true relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/overfitting2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='demo'></a>\n",
    "\n",
    "#### Load the Data\n",
    "\n",
    "---\n",
    "\n",
    "Today's [dataset](http://faculty.marshall.usc.edu/gareth-james/ISL/data.html) (`Advertising.csv`) is from the [ISLR website](http://faculty.marshall.usc.edu/gareth-james/ISL/index.html). \n",
    "\n",
    "Drop `Unnamed: 0` once you've loaded the csv into a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('http://faculty.marshall.usc.edu/gareth-james/ISL/Advertising.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "---\n",
    "\n",
    "1. Do we have any null values?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA: Plot a Heatmap of the Correlation Matrix\n",
    "---\n",
    "\n",
    "Heatmaps are an effective way to visually examine the correlational structure of your predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA: Use seaborn's `.pairplot()` method to create scatterplots for each of our columns\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='x-y'></a>\n",
    "\n",
    "## Create our features matrix (`X`) and target vector (`y`)\n",
    "---\n",
    "\n",
    "The following columns will be our features:\n",
    "- `TV`\n",
    "- `radio`\n",
    "- `newspaper`\n",
    "\n",
    "The `sales` column is our label: the column we're trying to predict.\n",
    "\n",
    "Create your `X` and `y` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['TV', 'radio', 'newspaper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"train-test-split\"></a>\n",
    "## Train/Test Split\n",
    "\n",
    "---\n",
    "\n",
    "What would a good model do?\n",
    "\n",
    "Predict values for y that are close to the true values for y - WITH DATA IT HASN'T SEEN!\n",
    "\n",
    "This is called _generalizing to new data_. \n",
    "\n",
    "### If your model doesn't generalize well, it's not good.\n",
    "\n",
    "---\n",
    "\n",
    "So far we've fit our models with ALL our data. So we haven't tried to generalize to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One popular method for evaluating how well our model generalizes is to break our data into 2 parts:\n",
    "\n",
    "This basic is called **train/test split**.\n",
    "\n",
    "Split our data into two parts:\n",
    "\n",
    "> **\"A Train Set\":** The subset of the data on which we fit our model.\n",
    "\n",
    "> **\"A Test Set\":** The subset of the data on which we EVALUATE the quality of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sklearn-tts'></a>\n",
    "\n",
    "## Scikit-Learn's `train_test_split` function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is tattoo worthy. üí™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's the default test set percentage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do the resulting variables look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are they rows now randomized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression model\n",
    "---\n",
    "\n",
    "Create a `LinearRegression` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit it to your **training data** (`X_train`, `y_train`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions on the test set. You are using X_test to predict what you think the y values should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "---\n",
    "\n",
    "Now use `.score` or a sklearn scoring function such as `mean_squared_error` to evaluate your model's performance on the test data. (how close are your y predictions to the ground truth values for y?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/I-am-something-of.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You care about the test set score. That's the important thing. \n",
    "\n",
    "The training set score gives you a hint to as to whether your model is underfitting or overfitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training set score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's rare that the MSE is higher for the train set than the test set. Usually the MSE is higher for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='cross-val-k-fold'></a>\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "\n",
    "---\n",
    "\n",
    "K-fold cross-validation takes the idea of a single train/test split and expands it to *multiple tests* across different train/test splits of your data.\n",
    "\n",
    "For example, if you determine your training set will contain 80 percent of the data and your testing set will contain the other 20 percent, you could have five different 80/20 splits in which the test set in each is a different set of observations. We have:\n",
    "- Five (K=5) training sets.\n",
    "- Five (K=5) corresponding testing sets.\n",
    "\n",
    "**K-fold cross-validation builds K models ‚Äî one for each train/test pair¬†‚Äî and evaluates those models on each respective test set.**\n",
    "\n",
    "### K-Fold Cross-Validation Visually with K = 3\n",
    "\n",
    "<img src=\"https://snag.gy/o1lLcw.jpg?convert_to_webp=true\" width=\"500\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation helps us understand how a model might perform in a variety of cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Folds Cross Validation in `sklearn`\n",
    "---\n",
    "\n",
    "Now let's try out k-fold cross-validation. scikit-learn provides a useful function\n",
    "\n",
    "With a regression problem, `cross_val_score` returns the $R^2$ for each of the orange sections above by default. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does each of those values represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What might be some nice summary statistics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTS with cross validation setup\n",
    "\n",
    "![](https://media.giphy.com/media/3bbcAXSqdmkqBQ22qw/giphy.gif)\n",
    "\n",
    "### Steps:\n",
    "1. Split dataset into test/holdout set and training set using `train_test_split`. Don't touch the holdout/test set until the end. ‚ö†Ô∏è\n",
    "1. Then do cross validation on the remaining 80% of the data (splitting it 5 different ways)\n",
    "1. Look at your validation scores (take the mean)\n",
    "1. Repeat for other models/hyperparameters\n",
    "1. Fit the best model on all the training data\n",
    "1. Score the best model on the holdout/test dataset\n",
    "\n",
    "\n",
    "### We need to use cross validation first and then use our test/holdout set \n",
    "We don't want information from the test set to leak into the training set when choosing our best model. \n",
    "\n",
    "!["
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do TTS again and THEN do cross validation on the remaining training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instantiate a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `cross_val_score` with the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get back an array with the R2 for each of the validation sets.\n",
    "\n",
    "Let's take the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's compare a different model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a lienar regression model without the intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# usually we'll do more interesting hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did that model do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the best model on ALL the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score the best model on the holdout/test set\n",
    "\n",
    "#### Now, let's see how our best model does with data it has never seen before (new data)! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does that compare to the R2 score on the whole training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between score on the training and test sets isn't large, so our model is not overfitting to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "### Compare three models to find the best performing model\n",
    "\n",
    "1. with only TV and Radio as predictors of Sales.\n",
    "1. with only Radio as a predictor of Sales.\n",
    "1. with only TV as a predictor of Sales.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Set up X and y\n",
    "1. Split the data with `train_test_split` with a random state of 1.\n",
    "1. Use `cross-val_score` to check which model works best.\n",
    "1. Fit the best-performing model on all the training data.\n",
    "1. Score the best-performing model on the hold-out/test dataset.\n",
    "1. Which model performed best?\n",
    "1. Use R2 as your evaluation metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Summary\n",
    "\n",
    "You will use `train_test_split()` all the time in your machine learning modeling workflow. You create your test/holdout set and put it to the side so it doesn't influence your model in any way.\n",
    "\n",
    "You will rarely use `cross_val_score()` directly with your training and validation sets. But you will use scikit-learn's `GridSearchCV` for your training and validation sets all the time. It does the same thing as `cross_val_score`, but is nicer to use. You'll learn about that soon. üëç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Check for understanding\n",
    "- What is the purpose of splitting a dataset into a training set and a test set?\n",
    "- What evaluation metric score do you care most about - the training set, validation set, or test set?\n",
    "- What is the advantage of using cross-validation to tune your hyperparameters instead of train_test_set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
